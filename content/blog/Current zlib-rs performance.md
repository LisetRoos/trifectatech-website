+++
title = "Current zlib-rs performance"
slug = "current-zlib-rs-performance"
authors = ["Folkert de Vries"]
date = "2024-08-08"

[taxonomies]
tags = ["zlib-rs", "data compression"] 

[extra]
source = "Trifecta Tech Foundation"
+++

Our [`zlib-rs`](https://github.com/memorysafety/zlib-rs) project implements a drop-in replacement for `libz.so`, a dynamic library that is widely used to perform gzip (de)compression.

<!-- more -->

Of course, `zlib-rs` is written in rust, and while we aim for a safe implementation, a crucial aspect of making this project successful is solid performance. The original zlib implementation does not make good use of modern hardware, and the bar for zlib performance is set by the [`zlib-ng`](https://github.com/zlib-ng/zlib-ng) fork of zlib. It drops some legacy support, and makes good use of modern CPU capabilities like SIMD instructions. It is not uncommon for zlib-ng to be 2X faster than stock zlib.

In order to be an attractive alternative to zlib, and make some system administrator go through the process of using our implementation, we must at least be close in performance to zlib-ng. In this post we'll see how the implementation performs today, and how we actually (try to) measure that performance.

## setup

In my experience, it is easiest to write a benchmark as a separate program. It is simpler to guarantee that the program won't optimize in invalid ways (e.g. by looking at the input it will receive) and we can use external tools on this program to inspect it easily.

Here is the main function we'll be using ([full source code](https://gist.github.com/folkertdev/7e3634b93b0b2c074f05d54819753a56)).

```rust
fn main() {
    let mut it = std::env::args();

    // skips the program name
    let _ = it.next().unwrap();

    let level: i32 = it.next().unwrap().parse().unwrap();

    let mut dest_vec = vec![0u8; 1 << 28];
    let mut dest_len = dest_vec.len();

    match it.next().unwrap().as_str() {
        "ng" => {
            let path = it.next().unwrap();
            let input = std::fs::read(path).unwrap();

            let err = compress_ng(&mut dest_vec, &mut dest_len, &input, level);
            assert_eq!(ReturnCode::Ok, err);
        }
        "rs" => {
            let path = it.next().unwrap();
            let input = std::fs::read(path).unwrap();

            let err = compress_rs(&mut dest_vec, &mut dest_len, &input, level);
            assert_eq!(ReturnCode::Ok, err);
        }
        other => panic!("invalid input: {other:?}"),
    }
}
```

The `compress_ng` and `compress_rs` functions are equivalent, except that they import the implementation of their respective library. Both are linked in statically.

The zlib-rs implementation relies heavily on instructions that are specific to your CPU. To make use of these instructions, and let the compiler optimize with the assumption that the instructions will exist, it is important to pass the `target-cpu=native` flag. The most convenient way of specifying this flag is in a `.cargo/config.toml` file like so:

```toml
[build]
rustflags = ["-Ctarget-cpu=native"]
```

We will use `silezia-small.tar` as our input data. This file is commonly used to benchmark compression algorithms. At 15mb, it is neither trivially small nor overly large.

We can then build and run the benchmark, picking the compression level and the implementation that should be used: 

```
> cargo build --release --example blogpost-compress
> ./target/release/examples/blogpost-compress 9 ng silesia-small.tar
> ./target/release/examples/blogpost-compress 9 rs silesia-small.tar
```

## The benchmarking process

For the actual benchmarking, I use a tool called [`poop`](https://github.com/andrewrk/poop/tree/main), the "performance optimization observability platform" by Andrew Kelley, of zig fame. I prefer it over tools like `hyperfine` because it reports extra statistics about the program.

Like `hyperfine`, `poop` takes a series of commands, will run them in a loop for some amount of time, and then reports the average time per iteration for each command. Note that we're not using `cargo run`, but instead call the binary in our target directory directly. That is extremely important, `cargo run` adds significant overhead!

```sh
poop \
    "./target/release/examples/blogpost-compress 1 ng silesia-small.tar" \
    "./target/release/examples/blogpost-compress 1 rs silesia-small.tar"
```
For end users, the most important metric is wall time: this is what e.g. `hyperfine` would report. But `poop` reports extra statistics that are helpful during development. For instance, the instruction count (number of instructions executed in total) is correlated with wall time, but usually has less noise. A high number of branch or cache misses gives direction to the search for optimization opportunities.

```
measurement          mean Â± Ïƒ        delta
wall_time           107ms Â± 2.78ms   âš¡ -  9.9% Â±  3.5%
peak_rss           26.6MB Â± 84.2KB     -  0.1% Â±  0.1%
cpu_cycles          382M  Â± 8.60M    âš¡ -  7.5% Â±  2.8%
instructions        642M  Â± 1.59K      -  0.0% Â±  0.0%
cache_references   7.62M  Â± 1.24M    âš¡ -  8.3% Â±  7.0%
cache_misses        334K  Â± 13.2K      +  0.1% Â±  1.3%
branch_misses      3.35M  Â± 7.51K      -  0.2% Â±  0.1%
```

Significant improvements are indicated by the lightning bolt emoji (and green colors in the terminal). I'll let you guess what emoji `poop` uses when performance gets worse. No emoji means no significant changes.

## Results

Let's compare the `ng` and `rs` implementations at three compresssion levels: 1, 6 and 9. Level 1 is the lowest level that does any work, level 6 is the default, and level 9 is the highest compression level. Intuitively, a higher compression level will try harder to compress your data: that takes more compute and will take longer.

**level 1**
```
Benchmark 1 (53 runs): ./target/release/examples/blogpost-compress 1 ng silesia-small.tar
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time          94.6ms Â± 1.87ms    92.1ms â€¦  103ms          3 ( 6%)        0%
  peak_rss           26.7MB Â± 83.2KB    26.6MB â€¦ 26.9MB          0 ( 0%)        0%
  cpu_cycles          332M  Â± 4.63M      324M  â€¦  347M           1 ( 2%)        0%
  instructions        468M  Â±  452       468M  â€¦  468M           0 ( 0%)        0%
  cache_references   6.72M  Â± 1.72M     4.17M  â€¦ 10.7M           0 ( 0%)        0%
  cache_misses        343K  Â± 16.3K      325K  â€¦  448K           2 ( 4%)        0%
  branch_misses      3.34M  Â± 12.0K     3.31M  â€¦ 3.36M           0 ( 0%)        0%
Benchmark 2 (44 runs): ./target/release/examples/blogpost-compress 1 rs silesia-small.tar
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time           114ms Â± 8.79ms     106ms â€¦  149ms          4 ( 9%)        ðŸ’©+ 20.2% Â±  2.6%
  peak_rss           26.7MB Â± 80.5KB    26.6MB â€¦ 26.9MB          0 ( 0%)          -  0.0% Â±  0.1%
  cpu_cycles          397M  Â± 21.2M      376M  â€¦  489M           4 ( 9%)        ðŸ’©+ 19.5% Â±  1.8%
  instructions        642M  Â± 1.22K      642M  â€¦  642M           2 ( 5%)        ðŸ’©+ 37.1% Â±  0.0%
  cache_references   7.76M  Â± 1.66M     4.99M  â€¦ 12.2M           0 ( 0%)        ðŸ’©+ 15.5% Â± 10.2%
  cache_misses        461K  Â±  209K      312K  â€¦ 1.08M           5 (11%)        ðŸ’©+ 34.4% Â± 16.7%
  branch_misses      3.32M  Â± 16.9K     3.30M  â€¦ 3.36M           0 ( 0%)          -  0.5% Â±  0.2%
```
**level 6**

```
Benchmark 1 (17 runs): ./target/release/examples/blogpost-compress 6 ng silesia-small.tar
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time           294ms Â± 8.09ms     286ms â€¦  320ms          1 ( 6%)        0%
  peak_rss           24.6MB Â± 80.8KB    24.4MB â€¦ 24.7MB          0 ( 0%)        0%
  cpu_cycles         1.13G  Â± 32.7M     1.09G  â€¦ 1.23G           1 ( 6%)        0%
  instructions       1.66G  Â± 1.02K     1.66G  â€¦ 1.66G           1 ( 6%)        0%
  cache_references   24.2M  Â± 9.59M     10.7M  â€¦ 54.0M           1 ( 6%)        0%
  cache_misses        352K  Â± 29.6K      333K  â€¦  463K           1 ( 6%)        0%
  branch_misses      9.24M  Â± 7.59K     9.23M  â€¦ 9.26M           0 ( 0%)        0%
Benchmark 2 (17 runs): ./target/release/examples/blogpost-compress 6 rs silesia-small.tar
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time           311ms Â± 6.39ms     303ms â€¦  324ms          0 ( 0%)        ðŸ’©+  5.7% Â±  1.7%
  peak_rss           24.6MB Â± 85.9KB    24.4MB â€¦ 24.7MB          0 ( 0%)          -  0.1% Â±  0.2%
  cpu_cycles         1.19G  Â± 24.7M     1.16G  â€¦ 1.25G           0 ( 0%)        ðŸ’©+  5.9% Â±  1.8%
  instructions       2.10G  Â±  349      2.10G  â€¦ 2.10G           0 ( 0%)        ðŸ’©+ 26.3% Â±  0.0%
  cache_references   25.7M  Â± 5.48M     19.3M  â€¦ 36.7M           0 ( 0%)          +  6.2% Â± 22.6%
  cache_misses        323K  Â± 19.0K      314K  â€¦  386K           2 (12%)         âš¡-  8.1% Â±  5.0%
  branch_misses      9.51M  Â± 10.3K     9.50M  â€¦ 9.53M           0 ( 0%)        ðŸ’©+  2.9% Â±  0.1%
```
**level 9**

```
Benchmark 1 (9 runs): ./target/release/examples/blogpost-compress 9 ng silesia-small.tar
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time           553ms Â± 14.2ms     541ms â€¦  581ms          0 ( 0%)        0%
  peak_rss           24.6MB Â± 80.3KB    24.5MB â€¦ 24.7MB          0 ( 0%)        0%
  cpu_cycles         2.15G  Â± 46.2M     2.11G  â€¦ 2.23G           0 ( 0%)        0%
  instructions       2.83G  Â± 1.65K     2.83G  â€¦ 2.83G           1 (11%)        0%
  cache_references   20.0M  Â± 12.4M     7.92M  â€¦ 42.2M           0 ( 0%)        0%
  cache_misses        384K  Â± 28.4K      358K  â€¦  433K           0 ( 0%)        0%
  branch_misses      23.1M  Â± 22.4K     23.1M  â€¦ 23.1M           0 ( 0%)        0%
Benchmark 2 (9 runs): ./target/release/examples/blogpost-compress 9 rs silesia-small.tar
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time           568ms Â± 13.4ms     555ms â€¦  597ms          0 ( 0%)          +  2.8% Â±  2.5%
  peak_rss           24.6MB Â± 95.4KB    24.4MB â€¦ 24.7MB          0 ( 0%)          +  0.0% Â±  0.4%
  cpu_cycles         2.22G  Â± 51.9M     2.17G  â€¦ 2.33G           0 ( 0%)          +  3.1% Â±  2.3%
  instructions       3.33G  Â±  571      3.33G  â€¦ 3.33G           0 ( 0%)        ðŸ’©+ 17.6% Â±  0.0%
  cache_references   25.3M  Â± 13.0M     10.2M  â€¦ 52.9M           0 ( 0%)          + 26.6% Â± 63.6%
  cache_misses        348K  Â± 27.1K      329K  â€¦  415K           1 (11%)         âš¡-  9.4% Â±  7.2%
  branch_misses      21.4M  Â± 21.7K     21.4M  â€¦ 21.4M           0 ( 0%)         âš¡-  7.3% Â±  0.1%
```

That's a lot of numbers. For users, the most important number is the wall time, where contrary to intuition zlib-rs is on-par with zlib-ng for the highest compression level, but much worse for the lowest compression level. That just reflects where we've spent our time so far: a lot of time has gone into compression level 9 where we already do well, almost none has gone into level 1 where we currently do comparatively poorly.

Note that the `instructions` number is structurally much higher for rust code, even when that is not reflected in the wall time. Most of this increase is bounds checks: these are comparisons branches that are always predicted correctly, so they have little runtime cost, but do count towards the number of executed instructions.

For completeness, here is a benchmark of decompression speed

```
Benchmark 1 (128 runs): ./target/release/examples/blogpost-uncompress ng silesia-small.tar.gz
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time          38.9ms Â±  949us    37.7ms â€¦ 44.9ms          4 ( 3%)        0%
  peak_rss           24.3MB Â± 77.7KB    24.1MB â€¦ 24.5MB          0 ( 0%)        0%
  cpu_cycles          119M  Â± 1.59M      118M  â€¦  129M          10 ( 8%)        0%
  instructions        219M  Â± 1.18K      219M  â€¦  219M           7 ( 5%)        0%
  cache_references   1.12M  Â± 20.6K     1.07M  â€¦ 1.24M           5 ( 4%)        0%
  cache_misses        394K  Â± 32.7K      349K  â€¦  477K           0 ( 0%)        0%
  branch_misses       984K  Â± 1.83K      981K  â€¦  995K           6 ( 5%)        0%
Benchmark 2 (123 runs): ./target/release/examples/blogpost-uncompress rs silesia-small.tar.gz
  measurement          mean Â± Ïƒ            min â€¦ max           outliers         delta
  wall_time          40.7ms Â±  935us    39.7ms â€¦ 45.2ms          1 ( 1%)        ðŸ’©+  4.8% Â±  0.6%
  peak_rss           24.3MB Â± 77.8KB    24.2MB â€¦ 24.5MB          0 ( 0%)          +  0.1% Â±  0.1%
  cpu_cycles          127M  Â± 2.18M      126M  â€¦  140M          24 (20%)        ðŸ’©+  6.5% Â±  0.4%
  instructions        345M  Â± 1.29K      345M  â€¦  345M           7 ( 6%)        ðŸ’©+ 57.6% Â±  0.0%
  cache_references   1.57M  Â± 17.0K     1.54M  â€¦ 1.67M           1 ( 1%)        ðŸ’©+ 40.5% Â±  0.4%
  cache_misses        574K  Â± 52.2K      513K  â€¦  640K           0 ( 0%)        ðŸ’©+ 45.7% Â±  2.7%
  branch_misses       988K  Â± 1.19K      986K  â€¦  991K           0 ( 0%)          +  0.4% Â±  0.0%
```

Being within ~5% of a highly optimized implementation is a good start, but clearly there is work left to be done.

Caveats apply: these results are on my specific x86_64 linux machine with AVX2 and with this specific input. We have not yet done extensive testing on other machines and other architectures.

## Conclusion

From the start of the zlib-rs project, we've been very mindful of performance. The architecture of the library is already geared towards performance (e.g. by doing all allocations up-front), and the zlib-ng implementation has SIMD implementation of algorithmic bottlenecks that we were able to adopt. 

Still, it is encouraging that this effort has paid of, and that we are extremely close to matching the performance of zlib-ng. There is still more work to do though: zlib-ng has made some recent further improvements, we suspect better data layout could give us further gains, and there are more instruction sets to support.

--- 

### Support us

We need your financial backing to maintain our software and start new projects. Please [get in touch with us](/support), if you are interested in financially supporting us.
